import os
import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. é…ç½®
load_dotenv()
DOCS_PATH = r"C:\Users\Administrator\Documents\EBOOK"  # ä½ çš„PDFç›®å½•
DB_PATH = "./chroma_db"  # å‘é‡æ•°æ®åº“ä¿å­˜ä½ç½®

def get_vectorstore():
    # ä½¿ç”¨æœ¬åœ° HuggingFace å‘é‡æ¨¡å‹ï¼ˆå…è´¹ï¼Œæ— é…é¢é™åˆ¶ï¼‰
    print("--- æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹... ---")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # å¦‚æœæ•°æ®åº“å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    if os.path.exists(DB_PATH):
        print("--- æ­£åœ¨ä»æœ¬åœ°åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“... ---")
        vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
        try:
            count = vectorstore._collection.count()
            print(f"--- å·²åŠ è½½æ•°æ®åº“ï¼Œå½“å‰åŒ…å« {count} æ¡æ–‡æ¡£åˆ†å— ---")
            if count > 0:
                return vectorstore
            print("--- âš ï¸ è­¦å‘Šï¼šæ•°æ®åº“ä¸ºç©ºï¼Œå‡†å¤‡é‡æ–°æ‰«æ... ---")
        except Exception as e:
            print(f"--- âš ï¸ æ£€æŸ¥æ•°æ®åº“çŠ¶æ€æ—¶å‡ºé”™: {e}ï¼Œå‡†å¤‡é‡æ–°æ‰«æ... ---")
    
    # å¦‚æœä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ™è¯»å–æ–‡ä»¶å¹¶åˆ›å»º
    print(f"--- æ­£åœ¨æ‰«æç›®å½• {DOCS_PATH} ä¸­çš„ PDF æ–‡ä»¶... ---")
    try:
        loader = DirectoryLoader(DOCS_PATH, glob="**/*.pdf", loader_cls=PyPDFLoader, show_progress=True)
        raw_documents = loader.load()
        
        if not raw_documents:
            print(f"--- âš ï¸ è­¦å‘Šï¼šåœ¨ {DOCS_PATH} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶ï¼ ---")
            # è¿”å›ä¸€ä¸ªç©ºçš„ vectorstore æˆ–è€…æŠ›å‡ºå¼‚å¸¸ï¼Œè¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç©ºçš„
            return Chroma(embedding_function=embeddings, persist_directory=DB_PATH)

        print(f"--- æ­£åœ¨å¯¹ {len(raw_documents)} é¡µæ–‡æ¡£è¿›è¡Œåˆ‡åˆ†... ---")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        documents = text_splitter.split_documents(raw_documents)
        
        print(f"--- æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶ä¿å­˜åˆ° {DB_PATH}... (å…± {len(documents)} ä¸ªåˆ†å—) ---")
        
        # æœ¬åœ°æ¨¡å‹å¯ä»¥ä¸€æ¬¡æ€§å¤„ç†ï¼Œæ— éœ€åˆ†æ‰¹
        vectorstore = Chroma.from_documents(
            documents=documents, 
            embedding=embeddings, 
            persist_directory=DB_PATH
        )
        
        print("--- å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼ ---")
        return vectorstore
    except Exception as e:
        print(f"--- âŒ åˆ›å»ºå‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e} ---")
        raise e

def format_docs(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨ä¸ºå­—ç¬¦ä¸²"""
    return "\n\n".join(doc.page_content for doc in docs)

def main():
    # è·å–æ•°æ®åº“
    try:
        vectorstore = get_vectorstore()
    except Exception as e:
        print(f"æ— æ³•åˆå§‹åŒ–æ•°æ®åº“: {e}")
        return

    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    
    # è®¾ç½® DeepSeek æ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆå›ç­”ï¼‰
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
    
    # è®¾ç½® RAG æç¤ºæ¨¡æ¿
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚ä¿æŒå›ç­”ç®€æ´å‡†ç¡®ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{question}"),
    ])
    
    # ä½¿ç”¨ LCEL æ„å»º RAG é“¾
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print("\n" + "="*50)
    print("ğŸš€ RAG ç³»ç»Ÿå·²å°±ç»ªï¼è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºã€‚")
    print("="*50)

    # äº¤äº’å¾ªç¯
    while True:
        query = input("\nâ“ è¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
        
        if query.lower() in ['exit', 'quit']:
            print("å†è§ï¼")
            break
        
        if not query:
            continue

        print("ğŸ§  æ€è€ƒä¸­...")
        try:
            # å…ˆè·å–ç›¸å…³æ–‡æ¡£ç”¨äºæ˜¾ç¤ºæ¥æº
            docs = retriever.invoke(query)
            
            print(f"\nğŸ” æ£€ç´¢åˆ° {len(docs)} æ¡ç›¸å…³è®°å½•:")
            for i, doc in enumerate(docs):
                source = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶'))
                page = doc.metadata.get('page', '?')
                # é¢„è§ˆå‰100ä¸ªå­—ç¬¦ï¼Œå»é™¤æ¢è¡Œç¬¦ä»¥ä¾¿æ˜¾ç¤º
                content_preview = doc.page_content[:100].replace('\n', ' ') + "..."
                print(f"   [{i+1}] {source} (P{page}): {content_preview}")
            print("-" * 50)
            
            # æ„å»ºå¹¶æ‰“å°å®Œæ•´çš„ prompt
            context = format_docs(docs)
            print("\n" + "="*50)
            print("ğŸ“ å‘é€ç»™å¤§æ¨¡å‹çš„å®Œæ•´ Prompt:")
            print("="*50)
            print("ã€ç³»ç»Ÿæç¤ºã€‘")
            print("ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚")
            print("å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚ä¿æŒå›ç­”ç®€æ´å‡†ç¡®ã€‚")
            print("\nä¸Šä¸‹æ–‡ï¼š")
            print(context)
            print("-"*50)
            print(f"ã€ç”¨æˆ·é—®é¢˜ã€‘{query}")
            print("="*50 + "\n")
            
            # è·å–ç­”æ¡ˆ
            answer = rag_chain.invoke(query)

            print(f"\nğŸ¤– AI å›ç­”:\n{answer}")
            
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()