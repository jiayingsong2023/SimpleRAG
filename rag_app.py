import os
import time
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# ReRank ç›¸å…³å¯¼å…¥
from sentence_transformers import CrossEncoder

# 1. é…ç½®
load_dotenv()
DOCS_PATH = r"C:\Users\Administrator\Documents\EBOOK"  # ä½ çš„PDFç›®å½•
DB_PATH = "./chroma_db"  # å‘é‡æ•°æ®åº“ä¿å­˜ä½ç½®

def get_vectorstore():
    # ä½¿ç”¨æœ¬åœ° HuggingFace å‘é‡æ¨¡å‹ï¼ˆå…è´¹ï¼Œæ— é…é¢é™åˆ¶ï¼‰
    print("--- æ­£åœ¨åŠ è½½æœ¬åœ° Embedding æ¨¡å‹... ---")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # å¦‚æœæ•°æ®åº“å·²å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    if os.path.exists(DB_PATH):
        print("--- æ­£åœ¨ä»æœ¬åœ°åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“... ---")
        vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
        try:
            count = vectorstore._collection.count()
            print(f"--- å·²åŠ è½½æ•°æ®åº“ï¼Œå½“å‰åŒ…å« {count} æ¡æ–‡æ¡£åˆ†å— ---")
            if count > 0:
                return vectorstore
            print("--- âš ï¸ è­¦å‘Šï¼šæ•°æ®åº“ä¸ºç©ºï¼Œå‡†å¤‡é‡æ–°æ‰«æ... ---")
        except Exception as e:
            print(f"--- âš ï¸ æ£€æŸ¥æ•°æ®åº“çŠ¶æ€æ—¶å‡ºé”™: {e}ï¼Œå‡†å¤‡é‡æ–°æ‰«æ... ---")
    
    # å¦‚æœä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ™è¯»å–æ–‡ä»¶å¹¶åˆ›å»º
    print(f"--- æ­£åœ¨æ‰«æç›®å½• {DOCS_PATH} ä¸­çš„ PDF æ–‡ä»¶... ---")
    try:
        loader = DirectoryLoader(DOCS_PATH, glob="**/*.pdf", loader_cls=PyPDFLoader, show_progress=True)
        raw_documents = loader.load()
        
        if not raw_documents:
            print(f"--- âš ï¸ è­¦å‘Šï¼šåœ¨ {DOCS_PATH} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶ï¼ ---")
            # è¿”å›ä¸€ä¸ªç©ºçš„ vectorstore æˆ–è€…æŠ›å‡ºå¼‚å¸¸ï¼Œè¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç©ºçš„
            return Chroma(embedding_function=embeddings, persist_directory=DB_PATH)

        print(f"--- æ­£åœ¨å¯¹ {len(raw_documents)} é¡µæ–‡æ¡£è¿›è¡Œåˆ‡åˆ†... ---")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        documents = text_splitter.split_documents(raw_documents)
        
        print(f"--- æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶ä¿å­˜åˆ° {DB_PATH}... (å…± {len(documents)} ä¸ªåˆ†å—) ---")
        
        # æœ¬åœ°æ¨¡å‹å¯ä»¥ä¸€æ¬¡æ€§å¤„ç†ï¼Œæ— éœ€åˆ†æ‰¹
        vectorstore = Chroma.from_documents(
            documents=documents, 
            embedding=embeddings, 
            persist_directory=DB_PATH
        )
        
        print("--- å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼ ---")
        return vectorstore
    except Exception as e:
        print(f"--- âŒ åˆ›å»ºå‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e} ---")
        raise e

def format_docs(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨ä¸ºå­—ç¬¦ä¸²"""
    return "\n\n".join(doc.page_content for doc in docs)

def rerank_documents(query, docs, top_n=5):
    """
    ä½¿ç”¨ CrossEncoder å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº
    """
    if not docs:
        return []
    
    print("--- æ­£åœ¨åŠ è½½ ReRank æ¨¡å‹ (BAAI/bge-reranker-base)... ---")
    # æ³¨æ„ï¼šä¸ºäº†æ€§èƒ½ï¼Œæ¨¡å‹åº”è¯¥åœ¨å…¨å±€åŠ è½½ï¼Œè¿™é‡Œä¸ºäº†ç®€å•æ”¾åœ¨å‡½æ•°é‡Œï¼ˆä¼šæœ‰é‡å¤åŠ è½½å¼€é”€ï¼‰
    # å®é™…ç”Ÿäº§ä¸­åº”è¯¥åœ¨ main æˆ–å…¨å±€å˜é‡ä¸­åŠ è½½ä¸€æ¬¡
    reranker = CrossEncoder("BAAI/bge-reranker-base")
    
    # å‡†å¤‡æ¨¡å‹è¾“å…¥: [[query, doc1], [query, doc2], ...]
    model_inputs = [[query, doc.page_content] for doc in docs]
    
    # è·å–åˆ†æ•°
    scores = reranker.predict(model_inputs)
    
    # å°†æ–‡æ¡£å’Œåˆ†æ•°ç»“åˆ
    doc_scores = list(zip(docs, scores))
    
    # æŒ‰åˆ†æ•°é™åºæ’åº
    doc_scores.sort(key=lambda x: x[1], reverse=True)
    
    # å–å‰ top_n
    top_docs = []
    for doc, score in doc_scores[:top_n]:
        # å¯ä»¥æŠŠåˆ†æ•°å­˜å…¥ metadataï¼Œæ–¹ä¾¿æŸ¥çœ‹
        doc.metadata['relevance_score'] = float(score)
        top_docs.append(doc)
        
    return top_docs

# å…¨å±€åŠ è½½ ReRank æ¨¡å‹ä»¥é¿å…é‡å¤åŠ è½½
print("--- æ­£åœ¨åˆå§‹åŒ– ReRank æ¨¡å‹... ---")
try:
    RERANKER = CrossEncoder("BAAI/bge-reranker-base")
except Exception as e:
    print(f"âš ï¸ æ— æ³•åŠ è½½ ReRank æ¨¡å‹: {e}")
    RERANKER = None

def rerank_documents_optimized(query, docs, top_n=5):
    if not docs or RERANKER is None:
        return docs[:top_n]
    
    model_inputs = [[query, doc.page_content] for doc in docs]
    scores = RERANKER.predict(model_inputs)
    doc_scores = list(zip(docs, scores))
    doc_scores.sort(key=lambda x: x[1], reverse=True)
    
    top_docs = []
    for doc, score in doc_scores[:top_n]:
        doc.metadata['relevance_score'] = float(score)
        top_docs.append(doc)
    return top_docs

def main():
    # è·å–æ•°æ®åº“
    try:
        vectorstore = get_vectorstore()
    except Exception as e:
        print(f"æ— æ³•åˆå§‹åŒ–æ•°æ®åº“: {e}")
        return

    # 1. åŸºç¡€æ£€ç´¢å™¨ (Recall)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 20})
    
    # è®¾ç½® DeepSeek æ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆå›ç­”ï¼‰
    llm = ChatOpenAI(
        model="deepseek-chat",
        api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com",
        temperature=0
    )
    
    # è®¾ç½® RAG æç¤ºæ¨¡æ¿
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚ä¿æŒå›ç­”ç®€æ´å‡†ç¡®ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{question}"),
    ])
    
    # ä½¿ç”¨ LCEL æ„å»º RAG é“¾
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨å¤„ç†æ£€ç´¢å’Œé‡æ’åºï¼Œæ‰€ä»¥é“¾åªè´Ÿè´£ç”Ÿæˆ
    generation_chain = (
        prompt
        | llm
        | StrOutputParser()
    )

    print("\n" + "="*50)
    print("ğŸš€ RAG ç³»ç»Ÿå·²å°±ç»ª (Recall + ReRank)ï¼è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºã€‚")
    print("="*50)

    # äº¤äº’å¾ªç¯
    while True:
        query = input("\nâ“ è¯·è¾“å…¥ä½ çš„é—®é¢˜: ").strip()
        
        if query.lower() in ['exit', 'quit']:
            print("å†è§ï¼")
            break
        
        if not query:
            continue

        print("ğŸ§  æ€è€ƒä¸­ (æ£€ç´¢ -> é‡æ’åº -> ç”Ÿæˆ)...")
        try:
            # 1. æ£€ç´¢ (Recall)
            initial_docs = retriever.invoke(query)
            print(f"   - åˆæ­¥æ£€ç´¢åˆ° {len(initial_docs)} æ¡è®°å½•")
            
            # 2. é‡æ’åº (ReRank)
            final_docs = rerank_documents_optimized(query, initial_docs, top_n=5)
            
            print(f"\nğŸ” æœ€ç»ˆæ£€ç´¢åˆ° {len(final_docs)} æ¡é«˜ç›¸å…³è®°å½• (å·²é‡æ’åº):")
            for i, doc in enumerate(final_docs):
                source = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶'))
                page = doc.metadata.get('page', '?')
                score = doc.metadata.get('relevance_score', 0.0)
                # é¢„è§ˆå‰100ä¸ªå­—ç¬¦
                content_preview = doc.page_content[:100].replace('\n', ' ') + "..."
                print(f"   [{i+1}] {source} (P{page}) [Score: {score:.4f}]: {content_preview}")
            print("-" * 50)
            
            # 3. ç”Ÿæˆ (Generation)
            context = format_docs(final_docs)
            
            print("\n" + "="*50)
            print("ğŸ“ å‘é€ç»™å¤§æ¨¡å‹çš„å®Œæ•´ Prompt:")
            print("="*50)
            print("ã€ç³»ç»Ÿæç¤ºã€‘")
            print("ä½ æ˜¯ä¸€ä¸ªé—®ç­”åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚")
            print("å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚ä¿æŒå›ç­”ç®€æ´å‡†ç¡®ã€‚")
            print("\nä¸Šä¸‹æ–‡ï¼š")
            print(context)
            print("-"*50)
            print(f"ã€ç”¨æˆ·é—®é¢˜ã€‘{query}")
            print("="*50 + "\n")
            
            # è°ƒç”¨ç”Ÿæˆé“¾
            answer = generation_chain.invoke({"context": context, "question": query})

            print(f"\nğŸ¤– AI å›ç­”:\n{answer}")
            
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()